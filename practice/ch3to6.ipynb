{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 신경망\n",
    "## 3.1 신경망의 구조\n",
    "### 3.1.1 신경망의 예\n",
    "- 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)으로 구성\n",
    "- 은닉층의 뉴런은 보이지 않음\n",
    "\n",
    "### 3.1.2 퍼셉트론 복습\n",
    "- 퍼셉트론(perceptron): 다수의 신호를 입력으로 받아 하나의 신호를 출력, 인지에 대한 기본단위\n",
    "$$y = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 \\leq 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 > 0) \\end{cases}$$\n",
    "- $b$: 편향(bias) , 뉴런이 얼마나 쉽게 활성화되는지 제어\n",
    "- $w_1, w_2$: 가중치(weight), 각 신호의 영향력 제어\n",
    "- $x_1, x_2$: 입력 신호\n",
    "- $y$: 출력\n",
    "\n",
    "\n",
    "\n",
    "### 3.1.3 활성화 함수등장\n",
    "- 활성화 함수(activation function): 입력 신호의 총합을 출력 신호로 변환하는 함수\n",
    "- 활성화 함수를 이용해 신경망의 표현력을 높일 수 있음\n",
    "- 단순 퍼셉트론은 단층 네트워크에서 step function(계단 함수)를 활성화 함수로 사용한 모델\n",
    "- 다층 퍼셉트론은 다층 네트워크에서 sigmoid, ReLU 등의 매끈한 활성화 함수 사용\n",
    "- unit-step function(활성함수)\n",
    "    - 비선형 함수로 임계값을 기준으로 0과 1로 나눔\n",
    "    - 임계값을 넘겨서 bias로 바꿈 hyposis함수 만듬\n",
    "\n",
    "$y = \\begin{Bmatrix}  0, w_1x_1 + w_2x_2 <=\\Theta\n",
    "\\\\ 1, w_1x_1 + w_2x_2 > \\Theta\n",
    "\\end{Bmatrix}$\n",
    "## 3.2 활성화 함수\n",
    "$$h(x) = \\begin{cases} 0 & (x \\leq 0) \\\\ 1 & (x > 0) \\end{cases}$$\n",
    "- 계단 함수(step function): 임계값을 경계로 출력이 바뀌는 함수\n",
    "\n",
    "### 3.2.1 시그모이드 함수\n",
    "$$h(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "- 시그모이드 함수(sigmoid function): 입력 신호의 총합을 0에서 1 사이의 값으로 변환\n",
    "- 변환된 신호를 다음 뉴런에 전달\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 계단 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    if x>0 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def step_fuction(x):\n",
    "    y = x>0\n",
    "    return y.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-1.0, 1.0, 2.0]) \n",
    "y = x>0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmswl\\AppData\\Local\\Temp\\ipykernel_7568\\3891679544.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(x>0, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x>0, dtype=np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = step_function(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 시그모이드 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894142, 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([-1.0, 1.0, 2.0])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4.] [1.         0.5        0.33333333]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([1.0, 2.0, 3.0])\n",
    "print(1.0 + t, 1.0/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 시그모이드 함수와 계단 함수 비교\n",
    "- 계단 함수: 0과 1 중 하나의 값만 출력\n",
    "- 시그모이드 함수: 연속적인 실수를 출력\n",
    "- 공통점 \n",
    "    - 입력이 작을 때 출력이 0에 가깝고, 입력이 커지면 출력이 1에 가까워짐\n",
    "    - 입력이 아무리 작거나 커도 출력은 0에서 1 사이\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 비선형함수\n",
    "- 선형 함수 : 출력이 입력의 상수배만큼 변하는 함수\n",
    "- 비선형 함수 : 직선 1개로는 그릴 수 없는 함수\n",
    "- 신경망에서 활성화 함수로 선형함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어짐 -> 어차피 상수배가 되어 은닉층 없이 구현될 수 있음\n",
    "- tanh, ReLU 등의 비선형 함수를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 ReLU 함수\n",
    "$$h(x) = \\begin{cases} x & (x > 0) \\\\ 0 & (x \\leq 0) \\end{cases}$$\n",
    "- ReLU 함수(Rectified Linear Unit): 입력이 0을 넘으면 그 입력을 그대로 출력, 0 이하일때 0을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 다차원 배열의 계산\n",
    "### 3.3.1 다차원 배열\n",
    "- 다차원 배열: 행렬(matrix)을 일반화한 개념\n",
    "- 배열의 차원 수 : np.ndim()\n",
    "- 배열의 형상(shape): np.shape()\n",
    "- 배열의 데이터 타입(dtype): np.dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (4,) 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([1, 2, 3, 4])\n",
    "print(np.ndim(A), A.shape, A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (3, 2)\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[1,2], [3,4], [5,6]])\n",
    "print(np.ndim(B), B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 행렬의 내적(행렬 곱)\n",
    "- 행렬의 내적: np.dot(A, B)\n",
    "- 행렬의 형상이 다를 경우 내적 불가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2], [3,4]])\n",
    "B = np.array([[5,6], [7,8]])\n",
    "print(np.dot(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22 28]\n",
      " [49 64]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "B = np.array([[1,2], [3,4], [5,6]])\n",
    "print(np.dot(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m C \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape, C\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "C = np.array([[1,2], [3,4]])\n",
    "print(A.shape, C.shape)\n",
    "print(np.dot(A, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 53 83]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2], [3,4], [5,6]])\n",
    "B = np.array([7,8])\n",
    "print(np.dot(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 신경망의 내적\n",
    "- 다차원 배열 내적 구하는 np.dot으로 한번에 결과 Y 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 11 17]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2])\n",
    "W = np.array([[1, 3, 5], [2, 4, 6]])\n",
    "Y = np.dot(X, W)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 3층 신경망 구현하기\n",
    "- 입력층(0층) : 2개의 뉴런\n",
    "- 첫 번째 은닉층(1층) : 3개의 뉴런\n",
    "- 두 번째 은닉층(2층) : 2개의 뉴런\n",
    "- 출력층(3층) : 2개의 뉴런\n",
    "\n",
    "### 3.4.1 표기법 설명\n",
    "- 가중치: $W^{(1)}_{1 2}$\n",
    "- $^{(1)}$ : 1층의 가중치\n",
    "- $_1$ : 다음층의 1번 째 뉴런\n",
    "- $_2$ : 앞층의 2번 째 뉴런"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 각 층의 신호 전달 구현하기\n",
    "- $a^{(1)}_1 = W^{(1)}_{11}x_1 + W^{(1)}_{21}x_2 + b^{(1)}_1$\n",
    "- $A^{(1)} = XW^{(1)} + B^{(1)}$\n",
    "- $A^{(1)} = (a^{(1)}_1, a^{(1)}_2, a^{(1)}_3), X = (x_1, x_2), B^{(1)} = (b^{(1)}_1, b^{(1)}_2, b^{(1)}_3)$\n",
    "- $W^{(1)} = \\begin{pmatrix} W^{(1)}_{11} & W^{(1)}_{21} & W^{(1)}_{31} \\\\ W^{(1)}_{12} & W^{(1)}_{22} & W^{(1)}_{32} \\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2,) (3,)\n",
      "[0.3 0.7 1.1] [0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "print(W1.shape, X.shape, B1.shape)\n",
    "A1 = np.dot(X, W1) + B1\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "print(A1, Z1)\n",
    "\n",
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "\n",
    "A2 = np.dot(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)\n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "\n",
    "A3 = np.dot(Z2, W3) + B3\n",
    "Y = identity_function(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 구현 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 출력층 설계하기\n",
    "- 분류 : 데이터가 어느 클래스에 속하는지 결정하는 문제\n",
    "- 회귀 : 입력 데이터에서 (연속적인) 수치를 예측하는 문제\n",
    "\n",
    "### 3.5.1 항등 함수와 소프트맥스 함수 구현하기\n",
    "- 항등 함수(identity function): 입력을 그대로 출력\n",
    "- 소프트맥스 함수(softmax function): $y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n}\\exp(a_i)}$\n",
    "- 소프트맥스 함수는 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받음\n",
    "- 지수화된 값의 합으로 나누어 확률로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "exp_a = np.exp(a)\n",
    "sum_exp_a = np.sum(exp_a)\n",
    "y = exp_a / sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 소프트맥스 함수 구현 시 주의점\n",
    "- 지수 함수를 사용하므로 오버플로 문제 발생 가능\n",
    "- 임의의 정수를 양쪽에 곱해 로그화\n",
    "- 일반적으로 입력 신호 중 최댓값을 빼주어 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.7 -1.1  0. ] [0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "np.exp(a) / np.sum(np.exp(a))\n",
    "c = np.max(a)\n",
    "print( a - c, np.exp(a - c) / np.sum(np.exp(a - c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오버플로우 막는 소프트맥스\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 소프트맥스 함수 특징\n",
    "- 출력은 0에서 1 사이의 실수\n",
    "- 출력의 총합은 1\n",
    "- 확률로 해석 가능\n",
    "- 출력층의 소프트맥스 함수는 생략 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691] 1.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "y = softmax(a)\n",
    "print(y, np.sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 출력층의 뉴런 수 정하기\n",
    "- 분류에서는 분류하고 싶은 클래스 수로 설정\n",
    "\n",
    "## 3.6 손글씨 숫자 인식\n",
    "- 순전파(forward propagation): 입력층에서 출력층으로 향하는 전달 과정\n",
    "- MNIST 데이터셋: 손글씨 숫자 이미지 집합\n",
    "    - 28x28 크기의 회색조 이미지로, 각 픽셀은 0에서 255까지의 값을 취함\n",
    "- load_mnist : normalize, flatten, one_hot_label 3가지 옵션으로 데이터셋을 불러올 수 있음\n",
    "    - normalize: 입력 이미지의 픽셀 값을 0.0 ~ 1.0 사이의 값으로 정규화\n",
    "    - flatten: 입력 이미지를 1차원 배열로 만듦\n",
    "    - one_hot_label: 레이블을 원-핫 인코딩으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from scratch.dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from scratch.dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label)\n",
    "\n",
    "print(img.shape)\n",
    "img = img.reshape(28, 28)\n",
    "print(img.shape)\n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 신경망의 추론 처리\n",
    "- 입력층 뉴런: 784개(28x28)\n",
    "- 출력층 뉴런: 10개(0~9)\n",
    "- 은닉층 뉴런: 50개, 100개 등 임의로 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"scratch/ch03/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f) # 실행 당시 객체 즉시 복원\n",
    "\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3 배치 처리\n",
    "- 배치(batch) : 하나로 묶은 입력 데이터\\\n",
    "     - 이미지 1장당 처리 시간을 줄이기 위해 여러장을 묶어 처리\n",
    "     - 데이터 전송이 병목으로 작용하는 경우 배치 처리 함으로써 버스에 주는 부하 줄임(입출력 문맥교환 줄어듬)\n",
    "- 다차원 배열의 대응하는 차원의 원소수가 일치함을 확인\n",
    "- 후에 이미지 여러장을 한번에 묶어 넘김\n",
    "- 배치 크기가 너무 큰 경우 : 메모리 부족, 속도 저하, gpu병렬처리 가능, 오버헤드 줄어들음\n",
    "- 배치 크기가 너무 작은 경우 : 데이터 로딩으로 오버헤드 증가속도 저하, gpu의 계산 효율이 떨어짐(병렬처리 불가), gradient descent의 방향이 불안정해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (784,) (784, 50) (50, 100) (100, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, _ = get_data()\n",
    "network = init_network()\n",
    "W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "\n",
    "print(x.shape, x[0].shape, W1.shape, W2.shape, W3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 6, 9] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(list(range(0, 10, 3)),list(range(0, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])\n",
    "y = np.argmax(x, axis=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1, 2, 1, 0])\n",
    "t = np.array([1, 2, 0, 0])\n",
    "print(y == t)\n",
    "np.sum(y == t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 정리\n",
    "- 신경망에서 활성화 함수로 시그모이드, ReLU와 같은 매끄러운 함수 사용\n",
    "- 넘파이의 다차원 배열로 신경망 효율적 구현\n",
    "- 기계학습 무제는 회귀와 분류로 나누어짐\n",
    "- 출력층의 활성와 함수는 회귀에서 주로 항등함수, 분류에서 소프트맥스 함수 사용\n",
    "- 분류에서 출력층의 뉴런 수는 분류하려는 클래스 수와 같게 설정\n",
    "- 배치는 입력 데이터를 묶은 것이며, 추론 처리를 위해 배치 단위로 진행하면 결과가 빨라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 신경망 학습\n",
    "## 4.1 데이터에서 학습\n",
    "- 가중치 매개변수의 값을 데이터로 보고 자동으로 결정\n",
    "- 선형 분리 가능 문제는 유한번의 학습을 통해 풀 수 있으나(퍼셉트론 수렴 정리), 비선형 분리 문제는 자동으로 학습 할 수 없음\n",
    "\n",
    "### 4.1.1 데이터 주도 학습\n",
    "- 고차원에 대한 작업을 할 시 깊은 신경망을 사용해야하는데 weight가 늘어나고 조건수가 늘어나므로 데이터가 많아야 한다\n",
    "- 데이터에서 특징(feature)을 추출하고, 이 특징의 패턴을 기계학습 기술로 학습\n",
    "    - 특징은 입력 데이터에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기\n",
    "    - 이미지의 특징은 주로 SIFT, SURF, HOG 등의 특징 사용(컴퓨터비전)\n",
    "- 사람이 생각한 특징을 사용하는 방법도 있으나, 신경망은 이미지에 포함된 중요한 특징까지도 기계가 스스로 학습\n",
    "- 신경망이 아닌 기계학습은 이미지를 벡터로 변환할 때 사용하는 특징을 여전히 사람이 설계해야함\n",
    "- 딥러닝(신경망)을 종단간 기계학습(end-to-end machine learning)으로 데이터 입력에서 결과 출력까지 사람의 개입없이 얻음\n",
    "\n",
    "### 4.1.2 훈련 데이터와 시험 데이터\n",
    "- 기계학습 문제는 훈련 데이터와 시험 데이터로 나누어 학습과 실험을 수행\n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가\n",
    "- 범용 능력: 아직 보지 못한 데이터로도 올바르게 판단하는 능력\n",
    "- 오버피팅(overfitting): 한 데이터셋에만 지나치게 최적화된 상태\n",
    "    - 오버피팅을 줄이기 위해 모델단순화, 데이터 추가, 드롭아웃, regulazation 등의 방법 사용 \n",
    "\n",
    "## 4.2 손실함수\n",
    "- 신경망 학습에서 신경망 성능의 나쁨을 나탄는 지표, 현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄\n",
    "- 손실함수의 값이 작을수록 좋음\n",
    "\n",
    "### 4.2.1 평균제곱오차\n",
    "$$E = \\frac{1}{2}\\sum_{k}(y_k - t_k)^2$$\n",
    "- $y_k$: 신경망의 출력\n",
    "- $t_k$: 정답 레이블\n",
    "- $k$: 데이터 차원 수\n",
    "- 추정값이 가우시안 분포를 따른다고 가정할 때, 평균제곱오차 함수를 최소화하는 것은 최대우도추정과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차\n",
    "$$E = -\\sum_{k}t_k\\log y_k$$\n",
    "- $y_k$: 신경망이 k라고 예측한 확률 출력\n",
    "- $t_k$: 정답 레이블, 정답에 해당하는 인덱스의 원소만 1(원핫인코딩)\n",
    "- 정답일 때 추정의 자연로그 계산\n",
    "- 정답일 때 출력이 전체 값을 정함\n",
    "- 엔트로피: 정보량의 기댓값, 확률이 낮을수록 어떤 정보일지 확실하지 않음\n",
    "- 손실함수는 최소화하기 위해 -붙임\n",
    "- 확률을 계속 곱하면 0에 수렵하므로 곱하기를 더하기로 바꾸는 log함수사용\n",
    "- 이진분류일 경우는p,q에서 q==1이 되는 값 찾기\n",
    "- 베르누이 분포에서는 mse보다 ㅡ로스엔트로피를 씀 -> sfotmax와 결이 맞으므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "print(cross_entropy_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습\n",
    "- 훈련 데이터 모두에 대한 손실함수의 합을 구하는 것은 현실적이지 않음\n",
    "- 데이터 일부를 추려 전체의 근사치로 이용\n",
    "- 미니배치(mini-batch): 훈련 데이터 중 일부를 무작위로 가져와 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from scratch.dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape, t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5669, 20293, 57867,  8008, 37275, 53587,  4410, 14693,   759,\n",
       "       10021])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 무작위로 빼내기\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 배치용 교차 엔트로피 오차\n",
    "- 데이터가 하나인 경우와 미니배치인 경우 모두 처리할 수 있도록 구현\n",
    "- 정답 레이블이 원-핫 인코딩이 아닌 경우도 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "    # return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size  : 원핫인코딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 손실함수 설정 이유\n",
    "- 정확도를 높게 하는 매개변수 값을 찾는 것이 목표\n",
    "- __정확도는 매개변수의 미분이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없음__ \n",
    "    - 정확도는 개선된다 하더라도 연속적인 변화가 아니므로 미분 불가하거나 매개변수를 약간 조정하는 정도에서 정확도는 개선되지 않음\n",
    "- 손실함수는 미분값이 0이 되지 않아 학습 가능\n",
    "\n",
    "## 4.3 수치 미분\n",
    "- 미분: 한순간의 변화량\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "### 4.3.1 미분\n",
    "- 수치 미분: 아주 작은 차분으로 미분을 구하는 것\n",
    "- 소수점 8자리 이하에서 반올림 오차 발생하므로 $h$를 작게 설정해야함\n",
    "-  x를 중심으로 전후의 차분을 계산하므로 중앙차분이라고함\n",
    "- 중앙차분은 진정한 접선(미분)과 값이 다름\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x-h)}{2h}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x)) / (h)\n",
    "\n",
    "# 수치미분(중앙차분)\n",
    "def numerical_diff2(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 수치 미분 예시\n",
    "- $y = 0.01x^2 + 0.1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGwCAYAAACHJU4LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBZElEQVR4nO3deVhU9eLH8c+wuwBuICCIuC+4b6mVlaaZlaaVmZWaLZYt5r3ltd9t8XZvtt2u1e2aLW5paZta2qolmrvgghvugAqoqAyLDDBzfn+YlAUKKJxZ3q/n4XmamTPD53hmOJ/OfM/3WAzDMAQAAOCEvMwOAAAAUBqKCgAAcFoUFQAA4LQoKgAAwGlRVAAAgNOiqAAAAKdFUQEAAE7Lx+wAl8LhcOjo0aMKDAyUxWIxOw4AACgDwzCUnZ2tiIgIeXld+JiJSxeVo0ePKioqyuwYAACgAlJTUxUZGXnBZVy6qAQGBko6u6JBQUEmpwEAAGVhtVoVFRVVvB+/EJcuKue+7gkKCqKoAADgYsoybIPBtAAAwGlRVAAAgNOiqAAAAKdFUQEAAE6LogIAAJwWRQUAADgtigoAAHBapheVI0eO6O6771bdunVVrVo1tW3bVps2bTI7FgAAcAKmTvh26tQp9erVS9dee62+/fZbhYSEaO/evapdu7aZsQAAgJMwtai88sorioqK0syZM4vvi4mJMTERAABwJqZ+9fPVV1+pS5cuuv322xUaGqqOHTvq/fffL3V5m80mq9V63g8AAHBfphaVAwcOaNq0aWrWrJm+//57Pfzww3r88cc1e/bsEpefMmWKgoODi3+4cjIAAO7NYhiGYdYv9/PzU5cuXbRmzZri+x5//HFt3LhRa9eu/dPyNptNNput+Pa5qy9mZWVxUUIAAC6z5bsydG2LUHl5XfzigeVhtVoVHBxcpv23qUdUwsPD1bp16/Pua9WqlVJSUkpc3t/fv/hKyVwxGQCAyvPJhhSNmb1JD82Nl8Nh2jENc4tKr169lJSUdN59e/bsUXR0tEmJAADApkMn9dzi7ZKk9pHBl/2ISnmYWlSefPJJrVu3Ti+99JL27dunjz/+WO+9957GjRtnZiwAADxWWtYZjZ2boEK7oRvbhmnctU1NzWNqUenatasWLlyoTz75RLGxsXrxxRc1depUjRgxwsxYAAB4pPxCu8Z+FK8TOTa1DAvUa7e1l8Vi3tEUyeTBtJeqPINxAABA6QzD0F8+26ovE46oVnVfff3olYqqU71SfpfLDKYFAADOYcbqQ/oy4Yi8vSx6565OlVZSyouiAgCAh1u197j+tXSnJOmZG1upV9N6Jif6DUUFAAAPduB4jsbNS5DDkIZ2itR9vRqZHek8FBUAADyUNb9Q98/ZJGt+kTo1rKWXhsSaPnj2jygqAAB4ILvD0GMfb9aB47kKDw7Qu/d0lr+Pt9mx/oSiAgCAB3rlu92K23NcAb5eev/eLgoNDDA7UokoKgAAeJjP4w/rvZUHJEmv395esQ2CTU5UOooKAAAeJCHllJ75MlGS9Nh1TXVTuwiTE10YRQUAAA+RlnVGD86JV4HdoX6t6+vJvs3NjnRRFBUAADzAmQK7Hpzz2/T4/xnWwdSLDZYVRQUAADdnGIae/mKbEo9kqU4NP71/bxfV8PcxO1aZUFQAAHBz7/y8T19vPSofL4v+N8J5pscvC4oKAABu7Icd6Xr9hz2SpH8MitUVjeuanKh8KCoAALip3elWjV+wRZJ0b49o3dW9obmBKoCiAgCAGzqZW6D7Z29SXoFdPZvU1bM3tTY7UoVQVAAAcDMFRQ49PDdeh0+dUXTd6nrnrk7y9XbNXb5rpgYAAKWa/PUOrT94UjX9ffT+vV1Uu4af2ZEqjKICAIAbmbP2kOatT5HFIr15Zwc1rx9odqRLQlEBAMBNxO05rslf75QkPdW/hfq0qm9yoktHUQEAwA3szcjWo/MSZHcYGtKpgR7u3cTsSJcFRQUAABeXmWPTfbM3KttWpK6NamvKkLayWJx/evyyoKgAAODCbEV2jZ0br9STZ9SwTnVNv6eL/H28zY512VBUAABwUYZhaNKXidp46JQCA3w0Y1QX1XHhM3xKQlEBAMBF/W/Ffn2ZcETeXha9c1cnNQ117TN8SkJRAQDABX23PU2vfZ8kSXrhlja6unmIyYkqB0UFAAAXk3g4q/gaPqN6NtI9V0SbG6gSUVQAAHAh6Vn5un/ORuUXOtS7eYj+PrCV2ZEqFUUFAAAXkVdQpDGzNyrDalPz+jX19l0d5eOi1/ApK/deOwAA3ITDYejJBVu046hVdWv46cORXRUU4Gt2rEpHUQEAwAW89kOSvt+RIT9vL713b2dF1aludqQqQVEBAMDJfbYpVdNW7JckvXpbO3WOrmNyoqpDUQEAwImtP5CpZxYmSpIeu66pBndsYHKiqkVRAQDASSVn5mrs3HgV2g0NbBuuJ/s2NztSlaOoAADghLLyCnXfrI06lVeo9pHBev329vLyco8LDZYHRQUAACdTUOTQ2Lnx2n88V+HBAXr/3i6q5uc+FxosD4oKAABO5NyFBtceyFRNfx/NGNVVoUEBZscyDUUFAAAn8vZP+/RFwuGzFxoc0UmtwoPMjmQqigoAAE5i0eYjeuPHPZKkFwfFqrebXmiwPCgqAAA4gfUHMvX059skSQ9d3Vh3dW9ociLnQFEBAMBk+4/n6MGP4lVgd+jGtmGaeENLsyM5DYoKAAAmysyxafTMjco6U6iODWvpjTs6eORpyKWhqAAAYJL8QrsemLNJKSfzFFWnmt6/t4sCfD3zNOTSUFQAADCBw2HoL59uVULKaQVX89XMUd1Ur6a/2bGcDkUFAAATvPp9kpYmpsnX26Lp93RW09CaZkdyShQVAACq2CcbUvRu3G9XQ76icV2TEzkvigoAAFUobs9x/X3RdknS+L7NdGvHSJMTOTdTi8oLL7wgi8Vy3k/LlpySBQBwT7vSrBo3L0F2h6EhnRroiT7NzI7k9HzMDtCmTRstW7as+LaPj+mRAAC47I6ePqPRMzcqx1akKxrX0ctD2sli4TTkizG9Ffj4+CgsLMzsGAAAVJqsM4UaNXOD0q35ahpaU9Pv7iI/H0ZflIXp/0p79+5VRESEGjdurBEjRiglJaXUZW02m6xW63k/AAA4M1uRXWM/iteejByFBvpr1uiuCq7ua3Ysl2FqUenevbtmzZql7777TtOmTdPBgwd11VVXKTs7u8Tlp0yZouDg4OKfqKioKk4MAEDZORyGJn6+TWsPZKqGn7dmju6qyNrVzY7lUiyGYRhmhzjn9OnTio6O1htvvKExY8b86XGbzSabzVZ822q1KioqSllZWQoK8uzLYAMAnM8r3+3WtBX75eNl0YxRXXU1V0OWdHb/HRwcXKb9t+ljVH6vVq1aat68ufbt21fi4/7+/vL3Z9Y+AIDz+2hdsqatODtXypQhbSkpFWT6GJXfy8nJ0f79+xUeHm52FAAAKuzHnRl6fvHZuVImXN9ct3dhqEJFmVpU/vrXvyouLk6HDh3SmjVrdOutt8rb21vDhw83MxYAABW2OeWUHvskQQ5DurNrlB67rqnZkVyaqV/9HD58WMOHD1dmZqZCQkJ05ZVXat26dQoJ4fAYAMD1HDqRqzGzNym/0KFrWoTon4NjmSvlEplaVObPn2/mrwcA4LLJzLFp5MwNOplboNgGQXrnrk7y8XaqERYuiX9BAAAu0ZkCu8bM3qTkzDxF1q6mGaO6qoa/U52v4rIoKgAAXAK7w9Bjn2zWltTTqlXdV7Pv66bQwACzY7kNigoAABVkGIZe+GqHlu3KkJ+Plz64t4uahNQ0O5ZboagAAFBB//1pnz5alyyLRZo6rIO6NKpjdiS3Q1EBAKAC5m9I0b9/3CNJeuHmNrqxLXOAVQaKCgAA5fTjzgw9szBRkjTu2iYa2bORuYHcGEUFAIByiE8+qUc/Pjuh2+2dI/XXfi3MjuTWKCoAAJTR3oxs3Tdrk2xFDl3XMlRThrRlQrdKRlEBAKAM0rLO6N4ZG5R1plAdG9ZiQrcqwr8wAAAXkZVXqJEzNigtK19NQmpoxsiuqubnbXYsj0BRAQDgAvIL7bp/zkbtychR/SB/zb6vm2rX8DM7lsegqAAAUIoiu0OPfbJZGw+dUmCAj2bf102RtaubHcujUFQAACiBYRh6dvEO/bjzt1lnW4YFmR3L41BUAAAowdRle/XJhhR5WaS37uyg7o3rmh3JI1FUAAD4g7nrkvXm8r2SpH8MitUNscw6axaKCgAAv/NNYpqeW7xdkvR4n2a6+4pokxN5NooKAAC/+mXvCY2fv0UOQxreLUpP9m1mdiSPR1EBAEDSltTTevCjTSqwOzQgNkz/HMyss86AogIA8Hj7jmVr1MwNyiuw68qm9TT1zg7y9qKkOAOKCgDAox0+lae7P9ig03mFah9VS9Pv6Sx/H2addRYUFQCAx8rMseneDzco3ZqvpqE1NXNUV9Xw9zE7Fn6HogIA8EjZ+YUaNXOjDpzIVYNa1fTRmG6qw9T4ToeiAgDwOPmFdj04J16JR7JUt4af5ozppvDgambHQgkoKgAAj1Jkd+jxTzZr7YFM1fT30azR3dQkpKbZsVAKigoAwGMYhqFJXybqh1+v3/P+vV3UNjLY7Fi4AIoKAMBjvPztbn0Wf1heFunt4R3VownX73F2FBUAgEd4N26/pq88IEl6eWg79W8TZnIilAVFBQDg9j7ZkKKXv90tSXrmxpa6o0uUyYlQVhQVAIBb+2rrUT2zMFGSNLZ3Ez14dROTE6E8KCoAALe1bGeGJizYIsOQRnRvqIk3tDA7EsqJogIAcEtr9p3QIx8nqMhh6NaODfTioFguMuiCKCoAALeTkHJK98/ZpIIih65vXV+v3dZOXlxk0CVRVAAAbmVXmlWjZvx2JeS3h3eUjze7O1fFlgMAuI0Dx3N0z4frZc0vUufo2nrv3s4K8OVKyK6MogIAcAtHTp/R3R+s14mcArUOD9KMUV1V3Y8rIbs6igoAwOUdy87XiPfX6WhWvhqH1NCcMd0UXM3X7Fi4DCgqAACXdjqvQPd+uEGHMvPUoFY1zbu/u+rV9Dc7Fi4TigoAwGXl2Io0auZG7U7PVmigvz5+oLvCg6uZHQuXEUUFAOCS8gvtemD2Jm1JPa1a1X019/7uiq5bw+xYuMwoKgAAl1NQ5NAj8xK09kCmavr7aPbobmpeP9DsWKgEFBUAgEspsjv0+Ceb9dPuY/L38dKHI7uofVQts2OhklBUAAAuw+4wNOHTrfpuR7r8vL30/r1d1L1xXbNjoRJRVAAALsHhMDTxi236autR+XhZ9L8RnXR18xCzY6GSUVQAAE7PMAw9u3i7Po8/LG8vi94e3lF9W9c3OxaqAEUFAODUDMPQi0t2ad76FFks0ht3tNeAtuFmx0IVcZqi8vLLL8tisWj8+PFmRwEAOAnDMPTq90masfqgJOmVIe00qEMDk1OhKjlFUdm4caOmT5+udu3amR0FAOBE3lq+T9NW7JckvTg4Vnd0jTI5Eaqa6UUlJydHI0aM0Pvvv6/atWubHQcA4CTejduv/yzbI0n6+8BWuueKaJMTwQymF5Vx48Zp4MCB6tu370WXtdlsslqt5/0AANzPzNUH9fK3uyVJT/VvofuvamxyIpjF1Otfz58/XwkJCdq4cWOZlp8yZYomT55cyakAAGb6eH2KJn+9U5L0+HVNNe7apiYngplMO6KSmpqqJ554QvPmzVNAQECZnjNp0iRlZWUV/6SmplZySgBAVfoi/rD+b1GiJOmhqxvryeubm5wIZrMYhmGY8YsXLVqkW2+9Vd7e3sX32e12WSwWeXl5yWaznfdYSaxWq4KDg5WVlaWgoKDKjgwAqESLtxzRkwu2yGFIo3o20vM3t5bFYjE7FipBefbfpn3106dPHyUmJp533+jRo9WyZUtNnDjxoiUFAOA+vtp6tLikDO8WpeduoqTgLNOKSmBgoGJjY8+7r0aNGqpbt+6f7gcAuK8l245q/PzNchjSsC5R+tfgtvLyoqTgLNPP+gEAeK5vEtP0xPyzR1Ju7xypKUMoKTifqWf9/NGKFSvMjgAAqCLfJqbpsU82y+4wNLRTpF4e2o6Sgj/hiAoAoMp9tz29uKQM6dhAr97WTt6UFJSAogIAqFI/7EjXox8nqMhhaFCHCL12e3tKCkpFUQEAVJllOzM07teScnP7CP2bkoKLoKgAAKrET7sz9PC8eBXaDQ1sF67/3NFePt7shnBhvEMAAJXu56RjGvtRwtmS0jZcbw7rQElBmfAuAQBUqrg9x/XQR/EqsDs0IDZMU++kpKDseKcAACrNyj3H9cCcTSoocqh/m/p6a3hH+VJSUA68WwAAleLn3cd0/68lpW+r+np7eCdKCsqNdwwA4LJbtjPj7Nc9RQ71a11f/xvRSX4+7HJQfk41My0AwPV9/+s8KYV2QwNiw/i6B5eEogIAuGzOTYtf5DB0U7tw/WdYB0oKLglFBQBwWSzZdlRPzN8i+68zzv77duZJwaWjqAAALtniLUf05IKzV0Ee0rEB0+LjsqHqAgAuyZcJh4tLyu2dIykpuKw4ogIAqLDPNqXq6S+2yTCkO7tG6aVb28qLkoLLiCMqAIAKmb8hpbikjOjekJKCSsERFQBAuc1bn6z/W7hdkjSyR7ReuKWNLBZKCi4/igoAoFzmrD2k5xbvkCSN7tVIz93UmpKCSkNRAQCU2fS4/Zry7W5J0gNXxeiZG1tRUlCpKCoAgIsyDENvLt+rqcv2SpLGXdtEf+3XgpKCSkdRAQBckGEYevm73Zoed0CS9FT/Fhp3bVOTU8FTUFQAAKVyOAxN/nqHZq9NliQ9e1NrjbkyxuRU8CQUFQBAiewOQ5O+3KZPNx2WxSL9c3CsRnSPNjsWPAxFBQDwJ4V2h/7y6VZ9tfWovCzS67e315BOkWbHggeiqAAAzmMrsuuxjzfrh50Z8vGy6M07O2pgu3CzY8FDUVQAAMXyC+166KN4xe05Lj8fL00b0Ul9WtU3OxY8GEUFACBJyrUV6f7Zm7T2QKaq+Xrr/Xu76Mpm9cyOBQ9HUQEAKOtMoUbP3KCElNOq6e+jGaO6qltMHbNjARQVAPB0mTk2jZy5QduPWBUU4KM5Y7qrQ1Qts2MBkigqAODR0rLO6O4P1mv/8VzVreGnj8Z0V+uIILNjAcUoKgDgoQ6eyNXdH6zXkdNnFB4coLn3d1eTkJpmxwLOQ1EBAA+0K82qez7coBM5NsXUq6GPxnRTZO3qZscC/oSiAgAeJj75lEbP3CBrfpFahQdpzn3dFBLob3YsoEQUFQDwIKv2HteDc+J1ptCuLtG19eGorgqu5mt2LKBUFBUA8BDfJqbp8fmbVWg3dHXzEL17dydV92M3AOfGOxQAPMCnm1L1ty+2yWFIA9uG6z/DOsjPx8vsWMBFUVQAwM19sOqA/rl0lyRpWJcovTSkrby9LCanAsqGogIAbsowDP1n2V69tXyvJOnBqxtr0oCWslgoKXAdFBUAcEMOh6F/LNmpWWsOSZKe6t9Cj1zThJICl0NRAQA3U1Dk0NOfb9WiLUclSS8OaqN7ejQyNxRQQRQVAHAjeQVFGjs3QSv3HJePl0Wv395egzs2MDsWUGEUFQBwEydzCzR61kZtTT2tar7e+t/dnXRti1CzYwGXhKICAG7g8Kk83Ttjgw4cz1Wt6r6aOaqrOjasbXYs4JKVu6js2rVL8+fP16pVq5ScnKy8vDyFhISoY8eO6t+/v4YOHSp/f6ZiBoCqsicjW/d+uEHp1nxFBAdozphuahoaaHYs4LKwGIZhlGXBhIQEPf300/rll1/Uq1cvdevWTREREapWrZpOnjyp7du3a9WqVbJarXr66ac1fvz4Si8sVqtVwcHBysrKUlAQlyUH4Hk2HTqp+2ZtlDW/SM1Ca2rOmG4KD65mdizggsqz/y7zEZWhQ4fqqaee0ueff65atWqVutzatWv15ptv6t///reeeeaZMocGAJTP8l0ZemRegmxFDnWOrq0PR3ZRrep+ZscCLqsyH1EpLCyUr2/ZL1xVluWnTZumadOm6dChQ5KkNm3a6LnnntOAAQPK9Ds4ogLAU322KVV/+zJRdoeh61qG6p27Oqman7fZsYAyKc/+u8wXeihrScnLyyvz8pGRkXr55ZcVHx+vTZs26brrrtOgQYO0Y8eOssYCAI9iGIbejduvpz7fJrvD0NBOkZp+T2dKCtxWha5I1adPHx05cuRP92/YsEEdOnQo8+vcfPPNuvHGG9WsWTM1b95c//rXv1SzZk2tW7euIrEAwK05HIb+tXSXXv52tyTpod6N9frt7eTrzcUF4b4q9O4OCAhQu3bttGDBAkmSw+HQCy+8oCuvvFI33nhjhYLY7XbNnz9fubm56tGjR4nL2Gw2Wa3W834AwBMUFDk04dMt+uCXg5Kk/7uxlSYNaMWU+HB7FZpHZenSpXrnnXd03333afHixTp06JCSk5O1ZMkS9evXr1yvlZiYqB49eig/P181a9bUwoUL1bp16xKXnTJliiZPnlyRyADgsqz5hRr7UbzW7M+Uj5dFr97WTkM6RZodC6gSZR5MW5JJkybplVdekY+Pj1asWKGePXuW+zUKCgqUkpKirKwsff755/rggw8UFxdXYlmx2Wyy2WzFt61Wq6KiohhMC8BtpWWd0eiZG7U7PVs1/Lz1v7s7q3fzELNjAZekPINpK1RUTp06pfvvv1/Lly/Xa6+9pri4OC1atEivvvqqHnnkkQoHl6S+ffuqSZMmmj59+kWX5awfAO4sKT1bo2ZuUFpWvkIC/TVzVFfFNgg2OxZwySplHpXfi42NVUxMjDZv3qyYmBg98MADWrBggR555BEtXbpUS5curVBw6ex4l98fNQEAT7R2f6Ye/GiTsvOL1CSkhmaN7qaoOtXNjgVUuQoNph07dqxWrlypmJiY4vuGDRumrVu3qqCgoMyvM2nSJK1cuVKHDh1SYmKiJk2apBUrVmjEiBEViQUAbuGrrUc1csYGZecXqUt0bX3xcE9KCjzWJY1RuVRjxozR8uXLlZaWpuDgYLVr104TJ07U9ddfX6bn89UPAHdiGIbeX3VAL31z9vTjAbFh+s+wDgrwZY4UuJdK+eonJSVFDRs2LHOII0eOqEGDBhdc5sMPPyzz6wGAO7M7DL24ZKdmrTkkSRrVs5Gevam1vL04/Riercxf/XTt2lUPPfSQNm7cWOoyWVlZev/99xUbG6svvvjisgQEAHeXX2jXox8nFJeU/7uxlZ6/mZICSOU4orJr1y7985//1PXXX6+AgAB17txZERERCggI0KlTp7Rz507t2LFDnTp10quvvlrhid8AwJOcyi3QA3M2aVPyKfl5e+n1O9rrlvYRZscCnEaZx6hs27ZNbdq0UUFBgb755hutWrVKycnJOnPmjOrVq6eOHTuqf//+io2NrezMxRijAsCVpWTmadSsDTpwPFeBAT56754u6tGkrtmxgEpXKfOoeHt7Kz09XSEhIWrcuLE2btyounXN/UBRVAC4qvjkU3pwziZl5hYoPDhAs0Z3U4uwQLNjAVWiUq6eXKtWLR04cECSdOjQITkcjktLCQAeaum2NA1/f50ycwvUJiJIi8b1oqQApSjzGJWhQ4eqd+/eCg8Pl8ViUZcuXeTtXfIpc+cKDQDgN4Zh6N24A3rlu7OnH/dtFao37+yoGv4VmnsT8Ahl/nS89957GjJkiPbt26fHH39cDzzwgAID+T8AACiLQrtDzy7arvkbUyVx+jFQVuWq8TfccIMkKT4+Xk888QRFBQDKwJpfqEfmJuiXfSfkZZGevam1RveKufgTAVTsWj8zZ8683DkAwC0dPpWn0TM3au+xHFX389bbwzuqT6v6ZscCXAZfjAJAJdmaelpjZm/SiRyb6gf568ORXP0YKC+KCgBUgu+2p2v8gs3KL3SoZVigZo7uqvDgambHAlwORQUALiPDMPTBqoN66dtdMgzpmhYh+u9dnVSTM3uACuGTAwCXSaHdoecW79AnG1IkSXdf0VAv3NxGPt5lnrIKwB9QVADgMjiVW6CH58Vr3YGTsljOXlhwzJUxslg4/Ri4FBQVALhE+47laMzsjUrOzFMNP2+9xZk9wGVDUQGAS7Byz3GN+zhB2flFiqxdTR+O7Mp0+MBlRFEBgAowDENz1ibrH0t2yu4w1CW6tt69p7Pq1fQ3OxrgVigqAFBOhXaHXvhqh+atPzto9rbOkfrXrbHy9yn5+mcAKo6iAgDlcDqvQI/MS9Ca/ZmyWKS/3dBSD17dmEGzQCWhqABAGe0/nqP7Z2/SwRO5quHnral3dtT1rRk0C1QmigoAlMEve0/okXnxsuYXqUGtavpgZBe1Cg8yOxbg9igqAHABhmHoo3XJmvz12UGznaNrazqDZoEqQ1EBgFLYiux6btEOLdiUKkka0rGBXhrSVgG+DJoFqgpFBQBKcMyar7Fz45WQclpeFmkig2YBU1BUAOAPtqSe1kMfbVKG1aagAB+9fVcn9W4eYnYswCNRVADgd76IP6xJCxNVUORQ09Caev/eLoqpV8PsWIDHoqgAgKQiu0MvfbNbM1YflCT1bVVf/xnWXoEBviYnAzwbRQWAxzuVW6BHP0nQ6n2ZkqTH+zTT+D7N5OXFeBTAbBQVAB5td7pVD8zZpNSTZ1Tdz1tv3NFeN8SGmx0LwK8oKgA81nfb0zTh063KK7Arqk41vX9vF7UMYxI3wJlQVAB4HIfD0NTle/XW8r2SpF5N6+q/wzupdg0/k5MB+COKCgCPkpVXqPELNuvnpOOSpDFXxmjSgJby8fYyORmAklBUAHiMHUez9PDcBKWczJO/j5deurWthnaONDsWgAugqADwCF8mHNakLxNlK3Ioqk41vXt3Z7WJCDY7FoCLoKgAcGsFRQ79c+lOzVmbLEm6pkWIpg7roFrVGY8CuAKKCgC3lWHN1yPzEhSffEoS86MAroiiAsAtrT+QqXEfb9aJHJsCA3w0dVgH9WlV3+xYAMqJogLArRiGoRmrD+mlb3bJ7jDUMixQ797dWY24Xg/gkigqANxGXkGRJn6RqK+3HpUkDeoQoSlD2qq6H3/qAFfFpxeAWzhwPEcPz01QUka2fLws+vvAVhrZs5EsFsajAK6MogLA5S3ZdlQTP9+m3AK7QgL99b8RndS1UR2zYwG4DCgqAFyWrciul5bu0uxfTz3uFlNH/x3eUaFBASYnA3C5UFQAuKTUk3l69OMEbT2cJUl65JommnB9c6bCB9wMRQWAy1m2M0MTPt0ia36Rgqv56j/D2uu6lpx6DLgjigoAl1Fkd+i1H5I0Pe6AJKlDVC39966Oiqxd3eRkACqLqcdIp0yZoq5duyowMFChoaEaPHiwkpKSzIwEwEmlZ+XrrvfXF5eUUT0b6dOHelBSADdnalGJi4vTuHHjtG7dOv34448qLCxUv379lJuba2YsAE7ml70nNPCtVdpw6KRq+vvofyM66YVb2sjPh/EogLuzGIZhmB3inOPHjys0NFRxcXG6+uqrL7q81WpVcHCwsrKyFBQUVAUJAVQlu8PQ2z/t1ZvL98owpFbhQfrfiE6KYZZZwKWVZ//tVGNUsrLOjt6vU6fk+Q9sNptsNlvxbavVWiW5AFS9Y9n5mrBgq37Zd0KSdGfXKL1wSxsF+HqbnAxAVXKaouJwODR+/Hj16tVLsbGxJS4zZcoUTZ48uYqTAahqK/cc14RPt+hEToECfL30r8FtNbRzpNmxAJjAab76efjhh/Xtt9/ql19+UWRkyX+QSjqiEhUVxVc/gJsotDv07x/26N24/ZKklmGB+u9dHdU0NNDkZAAuJ5f76ufRRx/VkiVLtHLlylJLiiT5+/vL39+/CpMBqCqpJ/P0+PzN2pxyWpJ09xUN9feBrfmqB/BwphYVwzD02GOPaeHChVqxYoViYmLMjAPAJN8mpunpL7YpO79IgQE+enVoOw1oG252LABOwNSiMm7cOH388cdavHixAgMDlZ6eLkkKDg5WtWrVzIwGoArkF9r14pKdmrc+RZLUsWEtvXVnR0XVYW4UAGeZOkaltMuvz5w5U6NGjbro8zk9GXBd+45l69GPN2t3erYk6eFfr9Xjy7V6ALfnMmNUnGQcL4AqZBiGPtt0WM9/tUNnCu2qV9NPb9zRQVc3DzE7GgAn5BSDaQF4Bmt+of6+cLu+2npUknRVs3r69x3tFRoYYHIyAM6KogKgSmw4eFJPLtiiI6fPyNvLor/0a66xVzeRl1fJXwEDgERRAVDJCu0OvbV8r975eZ8chtSwTnVNvbODOjWsbXY0AC6AogKg0hw8kavxC7Zoa+ppSdJtnSP1wi1tVNOfPz0Ayoa/FgAuu3MDZl/4eofyCuwKCvDRS0Pa6qZ2EWZHA+BiKCoALqtTuQV6ZmGivt1+dl6kKxrX0Rt3dFBELeZGAlB+FBUAl83qfSc04dMtyrDa5ONl0V/7t9ADVzWWNwNmAVQQRQXAJbMV2fXvH/bovZUHJEmN69XQm3d2VNvIYJOTAXB1FBUAl2RPRrbGz9+inWlWSdJd3Rvq7wNbqboff14AXDr+kgCoELvD0IxfDuq1H5JUUORQ7eq+emVoO/VrE2Z2NABuhKICoNxSMvP018+2asOhk5Kka1uE6JWh7RQaxAyzAC4vigqAMjMMQ59sSNU/l+5UXoFdNfy89febWuvOrlGlXmQUAC4FRQVAmRyz5uvpL7ZpRdJxSVK3RnX0+u3t1bBudZOTAXBnFBUAF/X11qN6dvF2nc4rlJ+Pl57q10L3XRnDaccAKh1FBUCpTuUW6NnF27VkW5okKbZBkN64o4Oa1w80ORkAT0FRAVCin5OOaeLn23Qs2yZvL4vGXdtUj13XVL7eXmZHA+BBKCoAzmPNL9RLS3dp/sZUSVLjkBr6zx0d1D6qlrnBAHgkigqAYj8nHdMzXyYqLStfkjS6VyNNvKGlAny9TU4GwFNRVAAoK69Q/1iyU18kHJYkRdetrleGttMVjeuanAyAp6OoAB5u2c4MPbMwUceybbJYpNE9Y/RU/xaq5sdRFADmo6gAHupUboEmf71Di7YclXT2QoKv3tZOXRrVMTkZAPyGogJ4oO+2p+nvi3boRI5NXhbpgasa68nrmzMWBYDToagAHiQzx6bnvtqhpb/Oi9IstKZeva2dOjasbXIyACgZRQXwAIZhaMm2ND3/1Q6dzC2Qt5dFY3s31uN9msnfh6MoAJwXRQVwc0dOn9Fzi7Zr+e5jkqSWYYF67bb2ahsZbHIyALg4igrgpuwOQ3PWHtLr3ycpt8AuX2+LHrmmqcZd21R+PswuC8A1UFQAN7Q73aq/fZGoLamnJUmdo2vr5SFt1Yxr9ABwMRQVwI3kF9r11vK9em/lARU5DAX6++jpAS01oltDeXGlYwAuiKICuIk1+0/omS8TdSgzT5LUv019Tb4lVmHBASYnA4CKo6gALu50XoH+tXSXPos/O/19/SB/Tb4lVjfEhpmcDAAuHUUFcFGGYejrbWn6x9c7dCKnQJJ09xUN9fQNLRUU4GtyOgC4PCgqgAs6eCJXzy3erlV7T0iSmobW1MtD2jL9PQC3Q1EBXEh+oV3TVuzXtLj9KihyyM/bS49c20QPX9OEidsAuCWKCuAiViQd0/Nf7VDyr4Nlr2pWT/8YFKuYejVMTgYAlYeiAji5tKwz+sfXO/Xt9nRJZwfLPndTG93YNkwWC6ccA3BvFBXASRXaHZq1+pD+s2yP8grs8vayaHTPRhp/fXPV9OejC8Az8NcOcEIbD53U3xduV1JGtqSzM8u+OChWrSOCTE4GAFWLogI4kcwcm17+dnfxnCi1q/tq0oBWuq1zJDPLAvBIFBXACRTaHZq7Lllv/LhH2flFkqTh3aL0dP+Wql3Dz+R0AGAeigpgstX7Tmjy1zu0JyNHktQ6PEgvDo5V5+jaJicDAPNRVACTpJ7M00vf7Co+m6d2dV/9tX8L3dm1obz5mgcAJFFUgCp3psCud+P26924/bIVOeRlke65IlpPXt9ctarzNQ8A/B5FBagihmHo2+3p+tfSXTpy+owk6YrGdfTCLW3UMoyzeQCgJBQVoArsTrdq8lc7tfZApiSpQa1q+r+BrTQglknbAOBCKCpAJTqZW6A3l+3R3PUpsjsM+ft4aWzvJhrbu4mq+XFtHgC4GIoKUAlsRXbNXnNIb/+0r/h04wGxYXrmxlaKqlPd5HQA4Dq8zPzlK1eu1M0336yIiAhZLBYtWrTIzDjAJTMMQ98kpqnvG3F66Zvdys4vUqvwIM27v7um3d2ZkgIA5WTqEZXc3Fy1b99e9913n4YMGWJmFOCSbU45pX8t3aVNyackSaGB/vpr/xYa2imS040BoIJMLSoDBgzQgAEDyry8zWaTzWYrvm21WisjFlAuh0/l6dXvkvTV1qOSpABfLz14dRM9dHVj1eDigQBwSVzqr+iUKVM0efJks2MAkqTs/EL9b8V+ffjLQRUUOWSxSEM6Ruqp/i0UFhxgdjwAcAsuVVQmTZqkCRMmFN+2Wq2KiooyMRE8UZHdoQWbUvXGD3uUmVsg6ex8KH8f2FqxDYJNTgcA7sWlioq/v7/8/f3NjgEPZRiGvtuertd+SNKB47mSpJh6NfTMja3Ut1Uo86EAQCVwqaICmGXN/hN65bskbU09LensdXke79NMI7pHy8/H1JPnAMCtUVSAC9hxNEuvfpekuD3HJUnV/bx1/5UxeuDqxgoM8DU5HQC4P1OLSk5Ojvbt21d8++DBg9qyZYvq1Kmjhg0bmpgMni4lM0///jFJi7ecPZPHx8uiu7o31GPXNVNIIF8/AkBVMbWobNq0Sddee23x7XMDZUeOHKlZs2aZlAqe7ESOTf/9aZ/mrU9Wod2QJN3cPkJ/ub65GtWrYXI6APA8phaVa665RoZhmBkBkCTl2Ir0waoDen/lAeUW2CVJVzWrp4k3tORMHgAwEWNU4NHyCoo0Z22ypsft16m8QklSu8hg/e2GlurZtJ7J6QAAFBV4pPxCu+atT9G0Fft0IufsXCiN69XQX/q10I1twzjVGACcBEUFHsVWZNeCjal65+d9yrCevRxDwzrV9USfZhrUIUI+3pxqDADOhKICj1Bod+izTYf135/26mhWviSpQa1qeuy6phraOVK+FBQAcEoUFbi1IrtDCzcf0Vs/7VXqyTOSpPpB/nr02qa6o2uU/H28TU4IALgQigrcUpHdoSXb0vTW8r06cOLsdPf1avrp4WuaakT3hgrwpaAAgCugqMCtFNodWphwRP9bsU+HMvMknZ3ufmzvJrqnR7Sq+/GWBwBXwl9tuIX8Qrs+iz+sd1fs15HTZ7/iqV3dV2OujNGoXjGq6c9bHQBcEX+94dLOFNj18YYUvbdyf/FZPPVq+uvBq2M0onu0alBQAMCl8VccLinHVqSP1ibrg1UHlJl7dh6U8OAAje3dRMO6RjEGBQDcBEUFLiUrr1Cz1hzSjNUHlXXm7EyyUXWq6ZFrmmpIpwacxQMAboaiApeQnpWvmasPat76FOXYiiRJjUNqaNw1TXVLhwjmQQEAN0VRgVPbm5Gt91Ye0KItR4qvZtyifqAeva6pbmwbLm8vproHAHdGUYHTMQxDm5JPaXrcfi3bdaz4/m4xdTS2d2Nd0zxUXhQUAPAIFBU4DYfD0I+7MjQ9br8SUk5LkiwWqX/rMD3Yu7E6NaxtbkAAQJWjqMB0tiK7FiYc0XurDujA8bOzyPp5e2lo5wa6/6rGahJS0+SEAACzUFRgmpO5BfpkQ4pmrTmk49ln50AJCvDR3VdEa1SvRgoNDDA5IQDAbBQVVLmk9GzNXH1QCzcfka3IIensHChjrozRnd0aMossAKAYewRUCYfD0M9JxzRj9UGt3pdZfH/bBsEa3auRbmoXIT8fTjEGAJyPooJKlWMr0uebUjVrzaHiiwR6WaQbYsN0X68YdY6uLYuFM3gAACWjqKBSpJ7M0+w1h7RgY6qyf52gLSjAR8O7NdQ9PaIVWbu6yQkBAK6AooLLxuEw9Mu+E5q7LlnLdmXIcXZ+NjUOqaHRPRtpSKdILhIIACgX9hq4ZKdyC/R5/GHNW59c/PWOJF3VrJ7uuzJGvZuFMEEbAKBCKCqoEMMwtDn1tOauS9aSbWkq+PXsnUB/Hw3p1EB3XxGtZvUDTU4JAHB1FBWUS15BkRZvOaq565K146i1+P42EUG6+4po3dI+gq93AACXDXsUlMnejGzNW5+iL+IPFw+O9fPx0k3twnXPFdHqEFWLs3cAAJcdRQWlyrUVaem2NC3YlKr45FPF90fXra67u0frts6Rql3Dz8SEAAB3R1HBeQzDUELKaX26MVVLth1VboFdkuTtZdF1LUN1zxXRurJpPQbHAgCqBEUFkqQTOTYtTDiiBZtSte9YTvH9MfVq6PYukbqtU6RCg7j2DgCgalFUPJjdYWjlnuNasDFVy3ZlqOjXiU8CfL10Y9twDesSpW4xdRh7AgAwDUXFA+3JyNbCzUe0MOGI0q35xfe3j6qlYV2idHP7cAUG+JqYEACAsygqHuKYNV9fbT2qLxOOaGfab6cV167uq1s7RmpY1yi1CGPeEwCAc6GouLFcW5F+2JmuLxOOaPW+E8VT2vt6W3RNi1AN6dhA17UKlb+Pt7lBAQAoBUXFzRTZHVq9P1OLNh/Rd9vTdabQXvxY5+jaGtyxgW5qG85pxQAAl0BRcQMOx9np7JduS9PX247qeLat+LFGdavr1o6RGtwxQtF1a5iYEgCA8qOouCjDMLT1cJaWbD2qbxLTdDTrt0Gxtav76ub2Ebq1YwNmjAUAuDSKigsxDEOJR7K0dFualmxL05HTZ4ofq+nvo+tb19fAtuHq3SJEvt5eJiYFAODyoKg4OcMwtOOoVUsT07R0W5pSTuYVP1bdz1t9W9XXwHbh6t08RAG+DIoFALgXiooTsjsMbU45pR92ZuiHHek6lPlbOanm660+rUJ1U7twXdMilHICAHBrFBUnkV9o1+p9J/TDjgwt352hEzkFxY8F+HrpupahGtg2Qte2DFF1PzYbAMAzsMcz0em8Av20+5h+2JGhlXuPK6/gt1OJAwN81KdlqPq1CVPv5iGq4c+mAgB4HvZ+VSwlM0/Ld2fohx0Z2nDopOznZmGTFB4coH6t66tfmzB1i6nDgFgAgMejqFSy/EK7Nhw8qRVJx7Ui6ZgOnMg97/GWYYHF5aRNRBCnEgMA8DsUlUqQejJPK/Yc14rdx7Rmf+Z5s8N6e1nUJbq2rm9dX/1ah6lh3eomJgUAwLlRVC4DW5FdGw+e0oqkY/o56Zj2Hz//qElooL+ubRGqa1qEqFezegriysQAAJSJUxSVd955R6+99prS09PVvn17vf322+rWrZvZsUpldxjaedSq1ftPaPW+E9p46KTyCx3Fj3t7WdS5YW1d0zJE1zQPVavwQL7SAQCgAkwvKgsWLNCECRP07rvvqnv37po6dar69++vpKQkhYaGmh1P0tlJ1w6cyNWafSe0el+m1h7IVNaZwvOWCQn01zXNQ3RNi1Bd2ayegqtx1AQAgEtlMQzDuPhilad79+7q2rWr/vvf/0qSHA6HoqKi9Nhjj+lvf/vbBZ9rtVoVHBysrKwsBQUFXdZc6Vn5Wr3vhFbvP6E1+zKVbs0/7/Ga/j66onEd9WxST72a1lPz+jU5agIAQBmUZ/9t6hGVgoICxcfHa9KkScX3eXl5qW/fvlq7du2flrfZbLLZfrsysNVqrZRcM1cf1OSvd553n5+3lzpH11avpnXVs2k9tWsQLB9OHwYAoFKZWlROnDghu92u+vXrn3d//fr1tXv37j8tP2XKFE2ePLnSc8U2CJaXRWrbIFg9m9ZTryb11KVRbaarBwCgipk+RqU8Jk2apAkTJhTftlqtioqKuuy/p2NULW1+rh/jTAAAMJmpRaVevXry9vZWRkbGefdnZGQoLCzsT8v7+/vL39+/0nP5eHspuBpf6wAAYDZT98Z+fn7q3Lmzli9fXnyfw+HQ8uXL1aNHDxOTAQAAZ2D6Vz8TJkzQyJEj1aVLF3Xr1k1Tp05Vbm6uRo8ebXY0AABgMtOLyrBhw3T8+HE999xzSk9PV4cOHfTdd9/9aYAtAADwPKbPo3IpKnMeFQAAUDnKs/9mxCgAAHBaFBUAAOC0KCoAAMBpUVQAAIDToqgAAACnRVEBAABOi6ICAACcFkUFAAA4LYoKAABwWqZPoX8pzk2qa7VaTU4CAADK6tx+uyyT47t0UcnOzpYkRUVFmZwEAACUV3Z2toKDgy+4jEtf68fhcOjo0aMKDAyUxWK5rK9ttVoVFRWl1NRUt7yOkLuvn8Q6ugN3Xz+JdXQH7r5+0uVfR8MwlJ2drYiICHl5XXgUiksfUfHy8lJkZGSl/o6goCC3feNJ7r9+EuvoDtx9/STW0R24+/pJl3cdL3Yk5RwG0wIAAKdFUQEAAE6LolIKf39/Pf/88/L39zc7SqVw9/WTWEd34O7rJ7GO7sDd108ydx1dejAtAABwbxxRAQAATouiAgAAnBZFBQAAOC2KCgAAcFoeXVTeeecdNWrUSAEBAerevbs2bNhwweU/++wztWzZUgEBAWrbtq2++eabKkpaPlOmTFHXrl0VGBio0NBQDR48WElJSRd8zqxZs2SxWM77CQgIqKLE5ffCCy/8KW/Lli0v+BxX2X7nNGrU6E/raLFYNG7cuBKXd/ZtuHLlSt18882KiIiQxWLRokWLznvcMAw999xzCg8PV7Vq1dS3b1/t3bv3oq9b3s9xZbrQOhYWFmrixIlq27atatSooYiICN177706evToBV+zIu/1ynSx7Thq1Kg/5b3hhhsu+rrOsh0vtn4lfSYtFotee+21Ul/T2bZhWfYR+fn5GjdunOrWrauaNWtq6NChysjIuODrVvQzfDEeW1QWLFigCRMm6Pnnn1dCQoLat2+v/v3769ixYyUuv2bNGg0fPlxjxozR5s2bNXjwYA0ePFjbt2+v4uQXFxcXp3HjxmndunX68ccfVVhYqH79+ik3N/eCzwsKClJaWlrxT3JychUlrpg2bdqcl/eXX34pdVlX2n7nbNy48bz1+/HHHyVJt99+e6nPceZtmJubq/bt2+udd94p8fFXX31Vb731lt59912tX79eNWrUUP/+/ZWfn1/qa5b3c1zZLrSOeXl5SkhI0LPPPquEhAR9+eWXSkpK0i233HLR1y3Pe72yXWw7StINN9xwXt5PPvnkgq/pTNvxYuv3+/VKS0vTjBkzZLFYNHTo0Au+rjNtw7LsI5588kl9/fXX+uyzzxQXF6ejR49qyJAhF3zdinyGy8TwUN26dTPGjRtXfNtutxsRERHGlClTSlz+jjvuMAYOHHjefd27dzceeuihSs15ORw7dsyQZMTFxZW6zMyZM43g4OCqC3WJnn/+eaN9+/ZlXt6Vt985TzzxhNGkSRPD4XCU+LgrbUNJxsKFC4tvOxwOIywszHjttdeK7zt9+rTh7+9vfPLJJ6W+Tnk/x1Xpj+tYkg0bNhiSjOTk5FKXKe97vSqVtI4jR440Bg0aVK7XcdbtWJZtOGjQIOO666674DLOvA0N48/7iNOnTxu+vr7GZ599VrzMrl27DEnG2rVrS3yNin6Gy8Ijj6gUFBQoPj5effv2Lb7Py8tLffv21dq1a0t8ztq1a89bXpL69+9f6vLOJCsrS5JUp06dCy6Xk5Oj6OhoRUVFadCgQdqxY0dVxKuwvXv3KiIiQo0bN9aIESOUkpJS6rKuvP2ks+/ZuXPn6r777rvgBThdbRuec/DgQaWnp5+3jYKDg9W9e/dSt1FFPsfOJisrSxaLRbVq1brgcuV5rzuDFStWKDQ0VC1atNDDDz+szMzMUpd15e2YkZGhpUuXasyYMRdd1pm34R/3EfHx8SosLDxvm7Rs2VINGzYsdZtU5DNcVh5ZVE6cOCG73a769eufd3/9+vWVnp5e4nPS09PLtbyzcDgcGj9+vHr16qXY2NhSl2vRooVmzJihxYsXa+7cuXI4HOrZs6cOHz5chWnLrnv37po1a5a+++47TZs2TQcPHtRVV12l7OzsEpd31e13zqJFi3T69GmNGjWq1GVcbRv+3rntUJ5tVJHPsTPJz8/XxIkTNXz48Ate5K2873Wz3XDDDZozZ46WL1+uV155RXFxcRowYIDsdnuJy7vydpw9e7YCAwMv+pWIM2/DkvYR6enp8vPz+1OBvtg+8twyZX1OWbn01ZNxcePGjdP27dsv+n1ojx491KNHj+LbPXv2VKtWrTR9+nS9+OKLlR2z3AYMGFD83+3atVP37t0VHR2tTz/9tEz/d+NqPvzwQw0YMEARERGlLuNq29CTFRYW6o477pBhGJo2bdoFl3W19/qdd95Z/N9t27ZVu3bt1KRJE61YsUJ9+vQxMdnlN2PGDI0YMeKig9adeRuWdR9hJo88olKvXj15e3v/aQRzRkaGwsLCSnxOWFhYuZZ3Bo8++qiWLFmin3/+WZGRkeV6rq+vrzp27Kh9+/ZVUrrLq1atWmrevHmpeV1x+52TnJysZcuW6f777y/X81xpG57bDuXZRhX5HDuDcyUlOTlZP/744wWPppTkYu91Z9O4cWPVq1ev1Lyuuh1XrVqlpKSkcn8uJefZhqXtI8LCwlRQUKDTp0+ft/zF9pHnlinrc8rKI4uKn5+fOnfurOXLlxff53A4tHz58vP+j/T3evTocd7ykvTjjz+WuryZDMPQo48+qoULF+qnn35STExMuV/DbrcrMTFR4eHhlZDw8svJydH+/ftLzetK2++PZs6cqdDQUA0cOLBcz3OlbRgTE6OwsLDztpHVatX69etL3UYV+Ryb7VxJ2bt3r5YtW6a6deuW+zUu9l53NocPH1ZmZmapeV1xO0pnj3J27txZ7du3L/dzzd6GF9tHdO7cWb6+vudtk6SkJKWkpJS6TSryGS5PYI80f/58w9/f35g1a5axc+dO48EHHzRq1aplpKenG4ZhGPfcc4/xt7/9rXj51atXGz4+Psbrr79u7Nq1y3j++ecNX19fIzEx0axVKNXDDz9sBAcHGytWrDDS0tKKf/Ly8oqX+eP6TZ482fj++++N/fv3G/Hx8cadd95pBAQEGDt27DBjFS7qL3/5i7FixQrj4MGDxurVq42+ffsa9erVM44dO2YYhmtvv9+z2+1Gw4YNjYkTJ/7pMVfbhtnZ2cbmzZuNzZs3G5KMN954w9i8eXPxGS8vv/yyUatWLWPx4sXGtm3bjEGDBhkxMTHGmTNnil/juuuuM95+++3i2xf7HFe1C61jQUGBccsttxiRkZHGli1bzvts2my24tf44zpe7L1e1S60jtnZ2cZf//pXY+3atcbBgweNZcuWGZ06dTKaNWtm5OfnF7+GM2/Hi71PDcMwsrKyjOrVqxvTpk0r8TWcfRuWZR8xduxYo2HDhsZPP/1kbNq0yejRo4fRo0eP816nRYsWxpdffll8uyyf4Yrw2KJiGIbx9ttvGw0bNjT8/PyMbt26GevWrSt+rHfv3sbIkSPPW/7TTz81mjdvbvj5+Rlt2rQxli5dWsWJy0ZSiT8zZ84sXuaP6zd+/Pjif4v69esbN954o5GQkFD14cto2LBhRnh4uOHn52c0aNDAGDZsmLFv377ix115+/3e999/b0gykpKS/vSYq23Dn3/+ucT35bl1cDgcxrPPPmvUr1/f8Pf3N/r06fOn9Y6Ojjaef/758+670Oe4ql1oHQ8ePFjqZ/Pnn38ufo0/ruPF3utV7ULrmJeXZ/Tr188ICQkxfH19jejoaOOBBx74U+Fw5u14sfepYRjG9OnTjWrVqhmnT58u8TWcfRuWZR9x5swZ45FHHjFq165tVK9e3bj11luNtLS0P73O759Tls9wRVh+/WUAAABOxyPHqAAAANdAUQEAAE6LogIAAJwWRQUAADgtigoAAHBaFBUAAOC0KCoAAMBpUVQAAIDToqgAAACnRVEBAABOi6ICAACcFkUFgNM4fvy4wsLC9NJLLxXft2bNGvn5+Z13+XgAnoOLEgJwKt98840GDx6sNWvWqEWLFurQoYMGDRqkN954w+xoAExAUQHgdMaNG6dly5apS5cuSkxM1MaNG+Xv7292LAAmoKgAcDpnzpxRbGysUlNTFR8fr7Zt25odCYBJGKMCwOns379fR48elcPh0KFDh8yOA8BEHFEB4FQKCgrUrVs3dejQQS1atNDUqVOVmJio0NBQs6MBMAFFBYBTeeqpp/T5559r69atqlmzpnr37q3g4GAtWbLE7GgATMBXPwCcxooVKzR16lR99NFHCgoKkpeXlz766COtWrVK06ZNMzseABNwRAUAADgtjqgAAACnRVEBAABOi6ICAACcFkUFAAA4LYoKAABwWhQVAADgtCgqAADAaVFUAACA06KoAAAAp0VRAQAATouiAgAAnNb/A6KlmBVGH7EOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20000099999917254 0.3000009999976072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1999999999990898, 0.2999999999986347)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5), numerical_diff(function_1, 10))\n",
    "numerical_diff2(function_1, 5), numerical_diff2(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분\n",
    "- 변수가 여럿인 함수의 미분\n",
    "$$f(x_0, x_1) = x_0^2 + x_1^2$$\n",
    "- $x_0$에 대한 편미분 $$\\frac{\\partial f}{\\partial x_0} = 2x_0$$\n",
    "\n",
    "- $x_1$에 대한 편미분 \n",
    "$$\\frac{\\partial f}{\\partial x_1} = 2x_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000099999994291\n",
      "8.00009999998963\n"
     ]
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# x0 = 3, x1 = 4일 때 x0에 대한 편미분\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))\n",
    "\n",
    "# x0 = 3, x1 = 4일 때 x1에 대한 편미분\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "print(numerical_diff(function_tmp2, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기\n",
    "- 모든 변수의 편미분을 벡터로 정리한 것\n",
    "- __기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법\n",
    "- 손실함수가 최솟값이 될때의 매개변수값\n",
    "- 기울기로 함수의 최솟값을 찾으려는 것\n",
    "- 기울기가 가르키는 방향아 최솟값인지 보장할 수 없음\n",
    "    - 그소값이나 안장점일 가능성이 있으므로 고원 같은 학습이 진행되지 않는 정체기에 빠질 수 있음\n",
    "- 학습률(learning rate): 매개변수 값을 갱신하는 양\n",
    "    - 너무 크면 발산, 너무 작으면 학습이 잘 되지 않음\n",
    "    - 적절한 학습률을 찾는 것이 중요\n",
    "\n",
    "- 학습률과 같은 매개변수는 __하이퍼파라미터__ 로 사람이 직접 설정해야함\n",
    "- 가중치와 편향은 기계가 자동으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경사하강법 구현\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x\n",
    "\n",
    "# 문제 : f(x0, x1) = x0^2 + x1^2의 최솟값 구하기\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))\n",
    "\n",
    "# 학습률이 너무 작은 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 신경망 기울기\n",
    "- 가중치 매개변수에 대한 손실함수의 기울기\n",
    "- 각원소에 대한 편미분으로 구함\n",
    "- __$\\frac{\\partial L}{\\partial W}$은 $W$와 형상이 같음 -> $2x3$ 행렬__ \n",
    "\n",
    "$$W = \\begin{pmatrix} w_{11} & w_{21} & w_{31} \\\\ w_{12} & w_{22} & w_{32} \\end{pmatrix}$$\n",
    "$$\\frac{\\partial L}{\\partial W} = \\begin{pmatrix} \\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{31}} \\\\ \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{32}} \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.12681524e-01  1.18641083e+00 -7.23499232e-01]\n",
      " [ 1.65308563e+00 -7.08928369e-04 -2.51264374e-02]]\n",
      "[ 1.30016815  0.71120846 -0.45671333] 0\n",
      "2.3035479367546854\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from scratch.common.functions import softmax, cross_entropy_error\n",
    "from scratch.common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)d\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p, np.argmax(p))\n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34732509  0.19273217 -0.54005726]\n",
      " [ 0.52098764  0.28909826 -0.81008589]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n",
    "\n",
    "# or\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기\n",
    "- 학습 :신경망의 가중치와 편항을 훈련 데이터에 적응하도록 조정하는 과정\n",
    "- 추론 : 컴퓨터가 알아낸 wiehgt와 bias를 바탕으로 새로운 입력에 출력 계산\n",
    "- 1단계 : 미니배치\n",
    "    - 훈련 데이터 중 일부를 무작위로 가져옴, 미니 배치의 손실함수 값을 줄임\n",
    "- 2단계 : 기울기 산출\n",
    "    - 미니 배치의 손실 함수 값을 줄이기 위해 가중치 매개변수의 기울기 구함\n",
    "    - 기울기는 손실함수를 가장 작게하는 뱡향 제시\n",
    "- 3단계 : 매개변수 갱신\n",
    "    - 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "- 4단계 : 반복\n",
    "\n",
    "- 미니배치로 무작위 선정한 데이터로 매개변수를 경사하강법을 사용해 갱신하면 확률적 경사하강법\n",
    "- 층이 깊어질수록 기울기 손실 문제 발생\n",
    "    - 해결 : 가중치 초기값을 적절히 설정, 활성화 함수로 ReLU 사용, 배치 정규화 사용, skip connection 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 2층 신경망 클래스 구현하기\n",
    "#### 변수\n",
    "pramas\n",
    "\n",
    "    - 신경망의 매개변수를 보관하는 딕셔너리 인스턴스 변수\n",
    "\n",
    "    - params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향\n",
    "\n",
    "    - params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향\n",
    "\n",
    "grads\n",
    "\n",
    "    - 기울기 보관하는 딕셔너리 인스턴스 변수(numerical_gradient() 메서드의 반환값)\n",
    "\n",
    "    - grads['W1']은 1번째 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기\n",
    "\n",
    "    - grads['W2']은 2번째 층의 가중치의 기울기, grads['b2']은 2번째 층의 편향의 기울기\n",
    "\n",
    "#### 메서드\n",
    "_ _ init _ _ (self, input_size, hidden_size, output_size, weight_init_std = 0.01)\n",
    "\n",
    "    - 초기화 수행  \n",
    "    - input_size: 입력층 뉴런 수  \n",
    "    - hidden_size: 은닉층 뉴런 수  \n",
    "    - output_size: 출력층 뉴런 수  \n",
    "    - weight_init_std: 가중치 초기화 시 정규분포의 스케일  \n",
    "\n",
    "predict(self, x)  \n",
    "    - 예측(추론) 수행  \n",
    "    - x: 이미지 데이터\n",
    "\n",
    "loss(self, x, t)  \n",
    "    - 손실함수의 값을 구함  \n",
    "    - x: 이미지 데이터, t: 정답 레이블\n",
    "\n",
    "accuracy(self, x, t)  \n",
    "    - 정확도를 구함\n",
    "  \n",
    "numerical_gradient(self, x, t)  \n",
    "    - 가중치 매개변수의 기울기를 수치 미분 방식으로 구함\n",
    "\n",
    "gradient(self, x, t)  \n",
    "    - 가중치 매개변수의 기울기 성능개선(오차역전법 체인룰)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from scratch.common.functions import *\n",
    "from scratch.common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100) (100,) (100, 10) (10,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m      6\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     39\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     41\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 42\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     44\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\dmswl\\DlPractice\\scratch\\common\\gradient.py:43\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m tmp_val \u001b[38;5;241m=\u001b[39m x[idx]\n\u001b[0;32m     42\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(tmp_val) \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m---> 43\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[0;32m     46\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 39\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     42\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 26\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 18\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[0;32m     19\u001b[0m z1 \u001b[38;5;241m=\u001b[39m sigmoid(a1)\n\u001b[0;32m     20\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, W2) \u001b[38;5;241m+\u001b[39m b2\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape, net.params['b1'].shape, net.params['W2'].shape, net.params['b2'].shape)\n",
    "\n",
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "print(grads['W1'].shape, grads['b1'].shape, grads['W2'].shape, grads['b2'].shape)\n",
    "# 수치미분으로 매개변수 기울기 계산시 오래걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현\n",
    "- 훈련 데이터 중 일부를 무작위로 가져와 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scratch.dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 시험 데이터로 평가\n",
    "- 훈련 데이터에만 지나치게 최적화되어 범용성을 잃는 __오버피팅__ 문제\n",
    "- 오버피팅을 방지하기 위해 시험 데이터로 정확도 평가\n",
    "- Epoch : 학습에서 훈련 데이터를 모두 소진했을 때의 횟수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scratch.dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 정리\n",
    "- 기계학습에서 사용하는 데이터셋은 훈련, 시험으로 나눔\n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험으로 평가\n",
    "- 손실함수로 지표하여 손실 함수의 값이 작아지는 방향으로 가중치 매개변수 갱신\n",
    "- 가중치 매개변수를 갱신할 시 가중치 매개변수의 기울기 이용하고 기울어진 방향으로 가중치의 값을 갱신하는 작업 반복\n",
    "- 아주 작은 값을 주었을 때의 차분으로 미분을 구하는 수치미분을 이용해 가중치 매개변수의 기울기 구함\n",
    "- 수치미분은 계산이 오래걸리지만 구현이 간단함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 오차역전파법\n",
    "- 가중치 매개변수의 기울기를 효율적으로 계산하는 방법\n",
    "\n",
    "## 5.1 계산 그래프\n",
    "- 계산 과정을 그래프로 나타낸 것\n",
    "- 그래프 자료구조, 복수의 노드와 엣지로 표현\n",
    "\n",
    "### 5.1.1 계산 그래프로 풀다\n",
    "- 문제1 : 1개에 100원인 사과를 2개 샀다. 소비세가 10%일 때 지불 금액은?\n",
    "    - 100원 * 2개 = 200원 -> 10% 소비세 20원 -> 220원\n",
    "- 문제2 : 1개에 100원인 사과를 2개, 1개에 150원인 귤을 3개 샀다. 소비세가 10%일 때 지불 금액은?\n",
    "    - 100원 * 2개 + 150원 * 3개 = 650원 -> 10% 소비세 65원 -> 715원\n",
    "1. 계산 그래프 구성\n",
    "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행\n",
    "\n",
    "### 5.1.2 국소적 계산\n",
    "- 계산 그래프의 특징은 각 노드의 계산이 국소적(관계된 작은범위) 계산이고, 노드끼리의 계산이 전파됨으로써 최종 결과를 얻음\n",
    "\n",
    "### 5.1.3 왜 계산 그래프로 푸는가\n",
    "- 국소적 계산으로 문제를 단순화할 수 있음\n",
    "- 중간 계산 결과를 모두 보관할 수 있음\n",
    "- 역전파를 통해 효율적으로 미분을 계산할 수 있음\n",
    "\n",
    "## 5.2 연쇄법칙\n",
    "- 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있음\n",
    "\n",
    "### 5.2.1 계산 그래프의 역전파\n",
    "- 역전파: 순전파와는 반대 방향으로 국소적 미분을 전달하는 것\n",
    "\n",
    "### 5.2.2 연쇄법칙\n",
    "- 합성 함수 : 여러 함수로 구성된 함수\n",
    "- 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있음\n",
    "$$z = t^2$$\n",
    "$$t = x + y$$\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx}$$\n",
    "$$\\frac{dz}{dt} = 2t$$\n",
    "$$\\frac{dt}{dx} = 1$$\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dt}\\frac{dt}{dx} = 2t * 1 = 2t = 2(x + y)$$\n",
    "\n",
    "\n",
    "### 5.2.3 연쇄법칙과 계산 그래프\n",
    "- 계산 그래프의 역전파는 연쇄법칙에 따라 국소적 미분을 곱하여 전달\n",
    "- __역전파가 하는 일은 연쇄법칙의 원리와 같음__ \n",
    "$$\\frac{\\partial z}{\\partial x} = 2(x+y)$$\n",
    "\n",
    "\n",
    "## 5.3 역전파\n",
    "### 5.3.1 덧셈 노드의 역전파\n",
    "- $z = x + y$ 에서 $\\frac{\\partial z}{\\partial x} = 1, \\frac{\\partial z}{\\partial y} = 1$\n",
    "- 상류에서 전해진 미분이 하류로 흘려짐\n",
    "- 덧셈 노드의 역전파는 1을 곱하기만 할 뿐 입력된 값을 그대로 다음 노드로 전달\n",
    "\n",
    "\n",
    "### 5.3.2 곱셈 노드의 역전파\n",
    "- $z = xy$ 에서 $\\frac{\\partial z}{\\partial x} = y, \\frac{\\partial z}{\\partial y} = x$\n",
    "- 상류의 값에 __순전파 때 입력 신호들을 서로 바꾼 값을 곱해 하류로 보냄__\n",
    "- 곱셈 역전파는 순방향 입력 신호의 값이 필요하므로 순전파의 입력 신호 변수에 저장함\n",
    "\n",
    "### 5.3.3 사과 쇼핑의 예\n",
    "- 사과 가격, 사과 개수, 소비세라는 세 변수가 최종 금액에 어떻게 영향을 주느냐 해결\n",
    "- 사과 개수에 대한 지불 금액 미분, 소비세에 대한 금액 미분, 사과 가격에 대한 지불 금액의 비분 계산\n",
    "\n",
    "## 5.4 단순한 계층 구현하기\n",
    "### 5.4.1 곱셈 계층\n",
    "- forward() 순전파와 backward() 역전파 처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self): # 변수 초기화\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y): # 순전파, x, y를 받아 곱셈 수행\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): # 역전파, 상류에서 넘어온 미분을 받아 x, y에 대한 미분 계산\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy\n",
    "    \n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "print(price)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self): # 초기화가 필요없음\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): # 상류에서 내려온 미분을 그대로 하류에 흘림\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    \n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "print(price)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU 계층\n",
    "- ReLU 수식\n",
    "    $y = x (x > 0), y = 0 (x \\leq 0)$\n",
    "- x에 대한 y의 미분 식\n",
    "    $y = \\frac{\\partial L}{\\partial y} (x > 0), y = 0 (x \\leq 0)$\n",
    "- 순전파에서 입력인 x가 0보다 크면 상류의 값을 그대로 하류에 보내고, 0보다 작을경우 하류로 신호를 보내지 않음(0을보냄)\n",
    "-> ReLU는 전기회로의 스위치와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "    \n",
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 sigmoid 계층\n",
    "- sigmoid 수식\n",
    "    $y = \\frac{1}{1 + \\exp(-x)}$\n",
    "\n",
    "- sigmoid 미분\n",
    "1. / 노드로 $y = \\frac{1}{x}$ 미분\n",
    "    \n",
    "    $\\frac{\\partial y}{\\partial x} = \\frac{1}{x^2} = -y^2$\n",
    "    \n",
    "    -> $-\\frac{\\partial L}{\\partial y}y^2$\n",
    "2. +노드로 상류의 값을 여과없이 보냄\n",
    "    \n",
    "    -> $-\\frac{\\partial L}{\\partial y}y^2$\n",
    "3. exp노드로 $y = exp(x)$ 연산 수행\n",
    "    \n",
    "    $\\frac{\\partial y}{\\partial x} = exp(x)$\n",
    "    \n",
    "    -> $-\\frac{\\partial L}{\\partial y}y^2exp(-x)$\n",
    "4. x 노드로 순전파 때 값을 서로 바꿔 곱합(여기서는 -1)\n",
    "    \n",
    "    -> $\\frac{\\partial L}{\\partial y}y^2exp(-x)$\n",
    "    \n",
    "따라서, $\\frac{\\partial L}{\\partial y}y^2exp(-x) = \\frac{\\partial L}{\\partial y}y(1-y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기\n",
    "### 5.6.1 Affine 계층\n",
    "- 신경망의 순전파 때 수행하는 행렬의 내적을 기하학에서는 어파인 변환(affine transformation)이라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2, 3) (3,)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2)\n",
    "W = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "print(X.shape, W.shape, B.shape)\n",
    "Y = np.dot(X, W) + B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Y를 활성화 함수로 변환해 다음 층으로 전파하는 것이 신경망 순전파의 흐름\n",
    "- 행렬을 사용한 역전파는 행렬의 원소마다 전개해보면 스칼라값을 사용한 위의 방식과 같은 순서로 생각하면됨\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}W^T$$\n",
    "$$\\frac{\\partial L}{\\partial W} = X^T\\frac{\\partial L}{\\partial Y}$$\n",
    "- $W^T$는 행렬의 전치를 의미\n",
    "\n",
    "- X와 $\\frac{\\partial L}{\\partial X}$의 형상은 같음\n",
    "- W와 $\\frac{\\partial L}{\\partial W}$의 형상은 같음\n",
    "$$X = \\begin{pmatrix} x_1 & x_2 \\\\ x_3 & x_4 \\end{pmatrix}$$\n",
    "$$\\frac{\\partial L}{\\partial X} = \\begin{pmatrix} \\frac{\\partial L}{\\partial x_1} & \\frac{\\partial L}{\\partial x_2} \\\\ \\frac{\\partial L}{\\partial x_3} & \\frac{\\partial L}{\\partial x_4} \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 배치용 Affine 계층\n",
    "- 입력인 x의 형상이 (N,2)가 되고, 2차원에 맞게 행렬계산 하면 됨\n",
    "- 편향 덧샘은 X * W 에 대한 편향이 각 데이터에 더해짐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "print(X_dot_W + B)\n",
    "\n",
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dY)\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.6.3 Softmax-with-Loss 계층\n",
    "- 입력값을 정규화하여 출력\n",
    "- 소프트맥스 계층에 손실함수 교차 엔트로피 오차 포함하여 softmax-with-loss 계층으로 구현\n",
    "- 소프트맥스 계층의 역전파는 $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$로 정답 레이블과의 차이를 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기\n",
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "- 전체\n",
    "    1. 미니배치\n",
    "    2. 기울기 계산 : 오차역전파법으로 기울기 계산\n",
    "    3. 매개변수 갱신\n",
    "    4. 1~3 반복\n",
    "\n",
    "### 5.7.2 오차역전파법으로 구한 기울기 검증하기\n",
    "- 이전 신경망에서 쌓이는 은닉층들을 Affine계층을 사용하고, softmaxwithloss 계층을 마지막 계층으로 사용\n",
    "- 가중치 매개변수의 기울기를 오차역전법으로 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from scratch.common.layers import *\n",
    "from scratch.common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "def predict(self, x):\n",
    "    for layer in self.layers.values():\n",
    "        x = layer.forward(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    return self.lastLayer.forward(y, t)\n",
    "\n",
    "def accuracy(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "    return accuracy\n",
    "\n",
    "def numerical_gradient(self, x, t):\n",
    "    loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "    return grads\n",
    "def gradient(self, x, t):\n",
    "    self.loss(x, t)\n",
    "\n",
    "    dout = 1\n",
    "    dout = self.lastLayer.backward(dout)\n",
    "\n",
    "    layers = list(self.layers.values())\n",
    "    layers.reverse()\n",
    "    for layer in layers:\n",
    "        dout = layer.backward(dout)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = self.layers['Affine1'].dW\n",
    "    grads['b1'] = self.layers['Affine1'].db\n",
    "    grads['W2'] = self.layers['Affine2'].dW\n",
    "    grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법을 사용한 학습 구현하기\n",
    "- 수치미분의 결과와 오차역전파 결과를 비교해 오차역전파법이 제대로 구현되었는지 확인하는 기울기 확인 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from scratch.dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))\n",
    "\n",
    "'''결과\n",
    "W1:2.161013431743266e-13\n",
    "b1:9.07425231386016e-13\n",
    "W2:8.007279117077183e-13\n",
    "b2:1.201261681202712e-10\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from scratch.dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 정리\n",
    "- 계산 그래프를 이용하면 시각적으로 과정을 파악할 수 있음\n",
    "- 계산 그래프의 노드는 국소적 계산으로 진행하고 국소적 계산을 조합해 전체 계산을 구함\n",
    "- 계산 그래프의 순전파는 통상의 계산을 수행하고 계산그래프의 역전파로 각 노드의 미분을 구할 수 있음\n",
    "- 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있음\n",
    "- 수치미분과 오차역전법의 결과를 비교하면 오차 역전법의 구현에 잘못이 없는지 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습관련 기술\n",
    "## 6.1 매개변수 갱신\n",
    "최적화 : 손실함수의 값을 가능한 낮추는 매개변수를 찾는 문제\n",
    "- 확률적 경사 하강법(SGD) : 매개변수의 기울기를 구해 기울어진 방향으로 매개변수 값을 갱신\n",
    "\n",
    "### 6.1.1 모험가 이야기\n",
    "- 땅의 기울기를 이용해 지금 서있는 장소에서 가장 가파른 길을 찾아 내려가는 것\n",
    "\n",
    "### 6.1.2 확률적 경사 하강법(SGD)\n",
    "- 매개변수 공간에서 손실함수의 기울기를 구해 기울어진 방향으로 매개변수 값을 갱신\n",
    "$$W \\leftarrow W - \\eta\\frac{\\partial L}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화를 담당하는 클래스를 구분하고 기능을 모듈화\n",
    "\n",
    "### 6.1.3 SGD의 단점\n",
    "- 비등방성 함수(anisotropy function)에서는 탐색 경로가 비효율적\n",
    "    - 비등방성 함수 : 방향에따라 성질이나 기울기가 달라짐\n",
    "- SGD의 단점을 개선한 모멘텀, AdaGrad, Adam 등의 기법이 있음\n",
    "\n",
    "### 6.1.4 모멘텀\n",
    "- 모멘텀 : 운동량을 뜻하는 단어로, 기울기 방향으로 힘을 받아 물체가 가속됨\n",
    "\n",
    "$$v \\leftarrow \\alpha v - \\eta\\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "- v는 물체의 속도, m은 물체의 무게, F는 힘\n",
    "- SGD와 비교했을 때 x축 방향으로 빠르게 다가가 지그재그의 움직임이 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 AdaGrad\n",
    "- 학습률 감소 : 학습률을 서서히 낮추는 방법\n",
    "- 매개변수의 원소마다 적응적으로 갱신 정도를 조정\n",
    "- 행렬의 원소별 곱셈 후 학슙를 더해 갱신\n",
    "$$h \\leftarrow h + \\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W}$$\n",
    "$$W \\leftarrow W - \\eta\\frac{1}{\\sqrt{h}}\\frac{\\partial L}{\\partial W}$$\n",
    "- 과거의 기울기를 계속 제곱해 더해 학습 진행시 갱신 강도가 약해짐\n",
    "- 무한히 학습시 순간 갱신량이 0이 되어 갱신하지 않음\n",
    "    - RMSProp : 지수이동평균을 사용해 과거 기울기의 반영 규모를 기하급수적으로 감소시킴, 과거의 기울기를 서서히 잊음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6 Adam\n",
    "- 공이 바닥을 구르는 듯한 모멘텀과 매개변수의 원소마다 적응적으로 갱신 정도를 조정하는 AdaGrad를 합침\n",
    "- 하이퍼파라미터의 편향 보정이 진행됨\n",
    "- 하이퍼파라미터를 3개 설정(학습률, 일차 모멘텀용 계수, 이차 모멘텀용 계수)\n",
    "\n",
    "\n",
    "### 6.1.7 어느 갱신 방법을 이용할 것인가?\n",
    "\n",
    "- 학습마다 장단점이 있어 적절한 기법을 선택해야함\n",
    "- 하이퍼파라미터의 설정이 중요\n",
    "\n",
    "### 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교\n",
    "- MNIST 데이터셋으로 각 최적화 기법을 비교\n",
    "- 학습 진도가 빠른 Adam이 가장 빠르게 학습\n",
    "- SGD가 가장 느림(하이퍼파라미터에 따라 달라짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
